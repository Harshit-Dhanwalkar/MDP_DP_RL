{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implication of Google DeepMind Lectures\n",
    "\n",
    "This notebook will discuss the implications of the Google DeepMind lectures on MDP and Dynamic Programming. We will cover the following topics:\n",
    "\n",
    "1. Introduction markov property, markov Reward Process(MRP), etc.\n",
    "2. Futher into Value function\n",
    "3. Bellman euation for MRPs, solving Bellman euations.\n",
    "4. Markov Decision Process(MDP)\n",
    "5. Policy, and more about Value fucntion\n",
    "6. Bellman Expectation Equation(BEE), BEE for $V^\\pi$, $Q^\\pi$ and $V_\\pi(2)$\n",
    "7.  Bellman Expectation Equation (Matrix form)\n",
    "8. Optimal Value Function\n",
    "9. Optimal Policy, finding optimal policy\n",
    "10. Bellman optimality Equation(BOE), BOE for $V_*$ and $Q_*$.\n",
    "11. Solving for Bellman Optimatity Equation\n",
    "\n",
    "Furthur more....\n",
    "1. Real-world applications and implications\n",
    "2. Future directions and challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Can furthur be appended for RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The Markov property is a fundamental concept in probability theory and stochastic processes. It states that the future behavior of a system depends only on its current state and is independent of its past history, given the present state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Def : State $S_t$ is markov iff\n",
    "$$P[S_{t+1}|S_t]=P[S_{t+1}|S_1...S_t]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_markov(state_sequence, P):\n",
    "    for i in range(len(state_sequence) - 1):\n",
    "        current_state = state_sequence[i]\n",
    "        next_state = state_sequence[i + 1]\n",
    "        if P[next_state] != P[current_state]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "state_sequence = [0, 1, 0, 1, 0]\n",
    "P = {0: [0, 1, 0, 0, 0],\n",
    "     1: [0, 1, 0, 0, 0],\n",
    "     2: [0, 0, 0, 1, 0],\n",
    "     3: [0, 0, 0, 0, 1],\n",
    "     4: [1, 0, 0, 0, 0]}\n",
    "\n",
    "result = is_markov(state_sequence, P)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "state_sequence = [0, 1, 2, 3, 4]\n",
    "P = {0: [0, 1, 0, 0, 0],\n",
    "     1: [0, 0, 1, 0, 0], \n",
    "     2: [0, 1, 0, 1, 0], \n",
    "     3: [0, 0, 0, 0, 1], \n",
    "     4: [1, 0, 0, 0, 0]}\n",
    "\n",
    "result = is_markov(state_sequence, P)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Transition matrix ($P$)\n",
    "State $s$ and successor state $s'$ and State transition probability $P_{ss'}=P[S_{t+1}=s'|S_t=s]$\n",
    "\n",
    "State transition matrix $P$ represents the probabilities of transitioning from one state to another in a Markov Decision Process (MDP). It is defined as:\n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "p_{11} & p_{12} & \\dots & p_{1n} \\\\\n",
    "p_{21} & p_{22} & \\dots & p_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "p_{n1} & p_{n2} & \\dots & p_{nn} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $p_{ij}$ represents the probability of transitioning from state $i$ to state $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the state transition probabilities\n",
    "P = {0: [0, 1, 0, 0, 0],\n",
    "    1: [0, 0, 1, 0, 0],\n",
    "    2: [0, 0, 0, 1, 0],\n",
    "    3: [0, 0, 0, 0, 1],\n",
    "    4: [1, 0, 0, 0, 0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Reward Process (MRPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an MRP, we have a set of states and a set of possible actions that can be taken in each state. When an action is taken in a state, the system transitions to a new state according to a transition probability distribution. Additionally, each state transition is associated with a reward.\n",
    "\n",
    "Formally, an MRP is defined as a tuple (S, P, R, γ), where:\n",
    "- S is the set of states.$$S = \\{s_1,s_2,...s_N\\}$$\n",
    "- P is the state transition probability matrix, where P(s, s') represents the probability of transitioning from state s to state s'.$$P_{ss'}=P[S_{t+1}=s'|S_t=s]$$\n",
    "- R is the reward function, where R(s, s') represents the reward obtained when transitioning from state s to state s'.$$R_{s}=E[R_{t+1}|S_t=s]$$\n",
    "- γ is the discount factor, which determines the importance of future rewards compared to immediate rewards. $$\\gamma \\in [0,1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal in an MRP is to find a policy that maximizes the expected cumulative reward over time. This can be achieved by solving the Bellman equations, which provide a way to compute the value function for each state. The value function represents the expected cumulative reward starting from a given state and \n",
    "following a specific policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Def : $G_t$ is the total discounted reward from the step function.$$G_t = R_{t+1}+\\gamma R_{t+2}+\\cdots=\\sum^{\\infty}_{k=0}\\gamma^kR_{t+k+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value immediate reward above delayed reward\n",
    "1. $\\gamma$ close to 0 leads to \"myopic\" evalution.\n",
    "2. $\\gamma$ close to 1 leads to \"Far-sighted\" evalution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0, 1, 0, 0, 0], 1: [0, 0, 1, 0, 0], 2: [0, 0, 0, 1, 0], 3: [0, 0, 0, 0, 1], 4: [1, 0, 0, 0, 0]}\n",
      "1\n",
      "0\n",
      "0\n",
      "[0, 1, 2, 3, 4]\n",
      "False\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# P = {0: [0, 1, 0, 0, 0], 1: [0, 0, 1, 0, 0], 2: [0, 0, 0, 1, 0], 3: [0, 0, 0, 0, 1], 4: [1, 0, 0, 0, 0]}\n",
    "action = 1\n",
    "state = 0\n",
    "next_state = 0\n",
    "state_sequence = [0, 1, 2, 3, 4]\n",
    "result = False\n",
    "\n",
    "transition_prob = P[state][next_state]\n",
    "\n",
    "print(P)\n",
    "print(action)\n",
    "print(state)\n",
    "print(next_state)\n",
    "print(state_sequence)\n",
    "print(result)\n",
    "print(transition_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transition probability from state 0 to state 1 when taking action 1 is 1.\n",
      "{0: [0, 1, 0, 0, 0], 1: [0, 0, 1, 0, 0], 2: [0, 0, 0, 1, 0], 3: [0, 0, 0, 0, 1], 4: [1, 0, 0, 0, 0]}\n",
      "1\n",
      "0\n",
      "1\n",
      "[0, 1, 2, 3, 4]\n",
      "False\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "state = 0\n",
    "action = 1\n",
    "next_state = 1\n",
    "\n",
    "transition_prob = P[state][next_state]\n",
    "print(f\"The transition probability from state {state} to state {next_state} when taking action {action} is {transition_prob}.\")\n",
    "print(P)\n",
    "print(action)\n",
    "print(state)\n",
    "print(next_state)\n",
    "print(state_sequence)\n",
    "print(result)\n",
    "print(transition_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function ($V(s)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function $V(s)$ gives the long-term value of state $s$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Def : State value function $V(s)$ of an MRP is the expected return starting from state $s$ $$V(s)=E[G_t|S_t=s]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function: [ 0.44199165 -0.62000928  0.42221191 -0.64198676  0.39779248]\n"
     ]
    }
   ],
   "source": [
    "# Define the reward function for each state\n",
    "R = np.array([1, -1, 1, -1, 0])\n",
    "\n",
    "# Define the discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Initialize the value function\n",
    "V = np.zeros(5)\n",
    "\n",
    "# Update the value function\n",
    "for _ in range(1000):\n",
    "    for s in range(5):  # Update the range to include the additional state\n",
    "        V[s] = R[s] + gamma * np.sum(P[s] * V)\n",
    "\n",
    "print(\"Value function:\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman equation for MRPs\n",
    "The value function can be decomposed into 2 parts:\n",
    "- immediate rewards $R_{t+1}$.\n",
    "- disounted value of successor state $\\gamma v(S_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "V(s) &= \\mathbb{E}[G_t|S_t=s] \\\\\n",
    "     &= \\mathbb{E}[R_{t+1}\\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots|S_t=s] \\\\\n",
    "     &= \\mathbb{E}[R_{t+1}\\gamma [R_{t+2} + \\gamma R_{t+3} + \\cdots]|S_t=s] \\\\\n",
    "     &= \\mathbb{E}[R_{t+1}\\gamma G_{t+1}|S_t=s]\\\\\n",
    "     &= \\mathbb{E}[R_{t+1}\\gamma v(s_{t+1})|S_t=s]\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Def : State value function $V(s)$ of an MRP is the expected return starting from state $s$ $$V(s)=E[G_t|S_t=s]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3306690738754695e-17\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "action = 1\n",
    "next_state = 0\n",
    "result = False\n",
    "s = 4\n",
    "state = 0\n",
    "state_sequence = [0, 1, 2, 3, 4]\n",
    "transition_prob = 0\n",
    "\n",
    "V_s_values = [R[action] * gamma * V[next_state] for next_state in state_sequence]\n",
    "V_s = np.mean(V_s_values)\n",
    "print(V_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bellman equation can be expressed using matrix\n",
    "The Bellman equation represents the relationship between the value of a state ($v$), the immediate reward ($R$) obtained in that state, the discount factor ($\\gamma$), and the value of the next state ($Pv$).\n",
    "\n",
    "In the equation, $v$ is a vector representing the values of all possible states in a given environment. $R$ is a vector containing the immediate rewards associated with each state. $\\gamma$  is a scalar value between 0 and 1 that determines the importance of future rewards compared to immediate rewards. $P$ is a transition matrix that describes the probability of transitioning from one state to another.\n",
    "\n",
    "we can write it as:\n",
    "\n",
    "$$v = R + \\gamma Pv$$\n",
    "\n",
    "Here, $v$, $R$, and $Pv$ are all column vectors. The matrix multiplication $Pv$ represents the weighted sum of the values of the next states, where the weights are determined by the transition probabilities in the matrix $P$. The scalar $\\gamma$  determines the discounting factor applied to the future rewards. this can be rewiten as :\n",
    "\n",
    "\\begin{bmatrix} V^{(1)} \\\\\\vdots \\\\V^{(n)}  \\end{bmatrix} = \\begin{bmatrix}   R_{1}\\\\ \\vdots \\\\ R_{n} \\end{bmatrix} + \\gamma \\begin{bmatrix}   P_{11} & \\cdots & P_{1n} \\\\    \\vdots &      & \\vdots \\\\    P_{n1} & \\cdots & P_{nn} \\end{bmatrix} \\begin{bmatrix} V^{(1)} \\\\  \\vdots \\\\  V^{(n)} \\end{bmatrix}\n",
    "\n",
    "To solve the Bellman equation, we can rearrange it as:\n",
    "\n",
    "$$(I - \\gamma P)v = R$$\n",
    "\n",
    "Where $I$ is the identity matrix. By solving this equation, we can find the values of all states in the environment, which can be used to make optimal decisions in a reinforcement learning setting.\n",
    "\n",
    "It's important to note that the dimensions of the matrices and vectors involved in the equation must be compatible for matrix operations. The number of rows in $P$ should be equal to the number of columns in $v$ and $R$. Additionally, the dimensions of $\\gamma Pv$ and $R$ should match for the addition operation to be valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal values of states: [ 0.44199165 -0.62000928  0.42221191 -0.64198676  0.39779248]\n"
     ]
    }
   ],
   "source": [
    "# Define the initial guess for state values (V)\n",
    "V = np.array([0, 0, 0, 0, 0])  # Placeholder values, you should replace this with your initial guess\n",
    "\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "# Convert transition probabilities dictionary to numpy array\n",
    "P_array = np.array([P[i] for i in range(len(P))])\n",
    "\n",
    "# Calculate the left-hand side of the Bellman equation: (I - gamma * P)\n",
    "I = np.eye(len(V))\n",
    "left_hand_side = I - gamma * P_array\n",
    "\n",
    "# Define the immediate rewards (R)\n",
    "R = np.array([1, -1, 1, -1, 0])  \n",
    "\n",
    "# Solve the linear equation system: (I - gamma * P) * V = R\n",
    "V_solution = np.linalg.solve(left_hand_side, R)\n",
    "\n",
    "print(\"Optimal values of states:\", V_solution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving Bellman equation\n",
    " - They are linear equations.\n",
    " - Solved directly by :\n",
    "       $$v= R+\\gamma Pv$$ $$(1-\\gamma P)v =R$$ $$v = (1-\\gamma P)^{-1}R$$\n",
    " - Computational complexcity is $O(n^3)$ for $n$ states.\n",
    " - Direct solution only possible for small MRPs.\n",
    " - There are many iterative methods for large MDPs, for example: \\\n",
    "       - Dynamic programming \\\n",
    "       - Monte-Carlo evalution \\\n",
    "       - Temporal-Difference learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal values of states: [ 0.44199165 -0.62000928  0.42221191 -0.64198676  0.39779248]\n"
     ]
    }
   ],
   "source": [
    "# Define the immediate rewards (R), transition probabilities (P), and discount factor (gamma)\n",
    "R = np.array([1, -1, 1, -1, 0])  # Immediate rewards for each state\n",
    "\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "# Calculate the inverse of (I - gamma * P)\n",
    "I = np.eye(len(P))  # Identity matrix matching the size of P\n",
    "inverse_matrix = np.linalg.inv(I - gamma * P)\n",
    "\n",
    "# Solve the Bellman equation: v = inverse_matrix * R\n",
    "V_solution = np.dot(inverse_matrix, R)\n",
    "\n",
    "print(\"Optimal values of states:\", V_solution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process (MDPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Decision process is a tuple $<S,A,P,R,\\gamma>$\n",
    "- S is a finite set of states. $$S = \\{s_1, s_2, \\ldots, s_n\\}$$\n",
    "- A is a finite set of actions. $$A = \\{a_1, a_2, \\ldots, a_m\\}$$\n",
    "- P is the state transition probability matrix, where $P(s, a, s')$ represents the probability of transitioning from state $s$ to state $s'$ when taking action $a$. $$P^a_{ss'} = P[S_{t+1}=s'|S_t=s, A_t=a]$$\n",
    "- R is the reward function, where $R(s, a)$ represents the reward obtained when taking action $a$ in state $s$. $$R^a_s = \\mathbb{E}[R_{t+1}|S_t=s, A_t=a]$$\n",
    "- $\\gamma$ is the discount factor, which determines the importance of future rewards compared to immediate rewards. It is a scalar value between 0 and 1, i.e, $$\\gamma \\in [0,1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block, class represents a Markov Decision Process (MDP).\n",
    "\n",
    "A Markov Decision Process is a mathematical framework used to model decision-making problems in situations where outcomes are partly random and partly under the control of a decision-maker. It consists of a set of states, a set of actions, transition probabilities, and rewards associated with state-action pairs.\n",
    "\n",
    "Attributes:\n",
    "- states (list): A list of all possible states in the MDP.\n",
    "- actions (list): A list of all possible actions in the MDP.\n",
    "- transition_probs (dict): A dictionary that maps state-action pairs to a list of tuples representing the next state and the corresponding transition probability.\n",
    "- rewards (dict): A dictionary that maps state-action pairs to the corresponding reward.\n",
    "\n",
    "Methods:\n",
    "- get_transition_prob(state, action, next_state): Returns the transition probability from a given state to the next state, given a specific action.\n",
    "- get_reward(state, action): Returns the reward associated with a given state-action pair.\n",
    "- get_possible_actions(state): Returns a list of possible actions from a given state.\n",
    "- get_possible_states(): Returns a list of all possible states in the MDP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovRewardProcess:\n",
    "    def __init__(self, states, actions, transition_probs, rewards, policy, discount_factor):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transition_probs = transition_probs\n",
    "        self.rewards = rewards\n",
    "        self.policy = policy\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_transition_prob(self, state, next_state):\n",
    "        transition_prob = 0.0\n",
    "        for action in self.actions:\n",
    "            if (state, action) in self.transition_probs:\n",
    "                action_prob = self.policy.get_action_probability(state, action)\n",
    "                transition_prob += action_prob * self.transition_probs[(state, action)].get(next_state, 0.0)\n",
    "        return transition_prob\n",
    "\n",
    "    def get_reward(self, state):\n",
    "        reward = 0.0\n",
    "        for action in self.actions:\n",
    "            if (state, action) in self.rewards:\n",
    "                action_prob = self.policy.get_action_probability(state, action)\n",
    "                reward += action_prob * self.rewards.get((state, action), 0.0)\n",
    "        return reward\n",
    "\n",
    "    def calculate_expected_reward(self):\n",
    "        expected_rewards = {}\n",
    "        for state in self.states:\n",
    "            expected_rewards[state] = self.get_reward(state)\n",
    "        return expected_rewards\n",
    "\n",
    "    def calculate_expected_transition_probs(self):\n",
    "        expected_transition_probs = {}\n",
    "        for state in self.states:\n",
    "            for next_state in self.states:\n",
    "                transition_prob = self.get_transition_prob(state, next_state)\n",
    "                if transition_prob > 0:\n",
    "                    if state not in expected_transition_probs:\n",
    "                        expected_transition_probs[state] = {}\n",
    "                    expected_transition_probs[state][next_state] = transition_prob\n",
    "        return expected_transition_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition probability: 0.5\n",
      "Reward: 1\n",
      "Possible actions in state 0 : ['action1']\n",
      "Possible states: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Define states, actions, transition probabilities, and rewards\n",
    "states = [0, 1, 2, 3, 4]\n",
    "actions = ['action1', 'action2', 'action3']  # Example actions\n",
    "transition_probs = {\n",
    "    (0, 'action1'): {1: 0.5, 2: 0.5},\n",
    "    (1, 'action2'): {2: 0.5, 3: 0.5},\n",
    "    (2, 'action3'): {3: 0.5, 4: 0.5},\n",
    "    # Define other transition probabilities as needed\n",
    "}\n",
    "rewards = {\n",
    "    (0, 'action1'): 1,\n",
    "    (1, 'action2'): -1,\n",
    "    (2, 'action3'): 1,\n",
    "    # Define other rewards as needed\n",
    "}\n",
    "\n",
    "# Create an instance of MarkovDecisionProcess\n",
    "mdp = MarkovDecisionProcess(states, actions, transition_probs, rewards)\n",
    "\n",
    "# Example usage:\n",
    "state = 0\n",
    "action = 'action1'\n",
    "next_state = 1\n",
    "transition_prob = mdp.get_transition_prob(state, action, next_state)\n",
    "reward = mdp.get_reward(state, action)\n",
    "possible_actions = mdp.get_possible_actions(state)\n",
    "possible_states = mdp.get_possible_states()\n",
    "\n",
    "print(\"Transition probability:\", transition_prob)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Possible actions in state\", state, \":\", possible_actions)\n",
    "print(\"Possible states:\", possible_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy  ($\\pi$)\n",
    "Def : A policy $\\pi$ is distribution over actions given states. $$\\pi(a|s)= P[A_t=a|S_t=s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given MDP : M = $<S,A,P,R,\\gamma>$ and policy $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, states, actions):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.policy = {}  # Dictionary to store the policy probabilities\n",
    "\n",
    "    def set_policy(self, state, action_probabilities):\n",
    "        if state not in self.policy:\n",
    "            self.policy[state] = {}\n",
    "        for action, probability in action_probabilities.items():\n",
    "            if action in self.actions:\n",
    "                self.policy[state][action] = probability\n",
    "\n",
    "    def get_action_probability(self, state, action):\n",
    "        if state in self.policy and action in self.policy[state]:\n",
    "            return self.policy[state][action]\n",
    "        else:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of taking action 'right' in state 1: 0.3\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "states = [0, 1, 2, 3]\n",
    "actions = ['left', 'right']\n",
    "\n",
    "# Create a policy instance\n",
    "policy = Policy(states, actions)\n",
    "\n",
    "# Define some action probabilities for each state\n",
    "action_probs_state0 = {'left': 0.4, 'right': 0.6}\n",
    "action_probs_state1 = {'left': 0.7, 'right': 0.3}\n",
    "action_probs_state2 = {'left': 0.2, 'right': 0.8}\n",
    "action_probs_state3 = {'left': 0.5, 'right': 0.5}\n",
    "\n",
    "# Set policies for each state\n",
    "policy.set_policy(0, action_probs_state0)\n",
    "policy.set_policy(1, action_probs_state1)\n",
    "policy.set_policy(2, action_probs_state2)\n",
    "policy.set_policy(3, action_probs_state3)\n",
    "\n",
    "# Get action probability for a specific state and action\n",
    "state = 1\n",
    "action = 'right'\n",
    "probability = policy.get_action_probability(state, action)\n",
    "print(f\"Probability of taking action '{action}' in state {state}: {probability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- State sequence $s_1, s_2 \\cdots $ is markov process $<S,P^\\pi>$\n",
    "- The state & reward sequence $S_1,R_2,S_2, \\cdots $ is a markov Reward process $<S,P^\\pi,R^\\pi, \\gamma>$\n",
    "- $$P^\\pi_{ss'}=\\sum_{a \\in A}\\pi(a|s)P^a_{ss'}$$\n",
    "$$R^\\pi_{s}=\\sum_{a \\in A}\\pi(a|s)R^a_{s}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected rewards: {0: 0.5, 1: 2.3, 2: 0.0}\n",
      "Expected transition probabilities: {0: {0: 0.25, 1: 0.25}, 1: {1: 0.48999999999999994, 2: 0.51}, 2: {0: 0.36000000000000004, 2: 0.54}}\n"
     ]
    }
   ],
   "source": [
    "# Define states, actions, transition probabilities, rewards, policy, and discount factor\n",
    "states = [0, 1, 2]\n",
    "actions = ['a', 'b']\n",
    "transition_probs = {\n",
    "    (0, 'a'): {0: 0.5, 1: 0.5},\n",
    "    (1, 'a'): {1: 0.7, 2: 0.3},\n",
    "    (1, 'b'): {2: 1.0},\n",
    "    (2, 'a'): {0: 0.4, 2: 0.6}\n",
    "}\n",
    "rewards = {\n",
    "    (0, 'a'): 1,\n",
    "    (1, 'a'): 2,\n",
    "    (1, 'b'): 3,\n",
    "    (2, 'a'): 0\n",
    "}\n",
    "discount_factor = 0.9\n",
    "\n",
    "# Define a policy\n",
    "class Policy:\n",
    "    def __init__(self):\n",
    "        self.policy = {\n",
    "            0: {'a': 0.5, 'b': 0.5},\n",
    "            1: {'a': 0.7, 'b': 0.3},\n",
    "            2: {'a': 0.9, 'b': 0.1}\n",
    "        }\n",
    "\n",
    "    def get_action_probability(self, state, action):\n",
    "        return self.policy[state][action]\n",
    "\n",
    "policy = Policy()\n",
    "\n",
    "# Create an instance of MarkovRewardProcess\n",
    "mrp = MarkovRewardProcess(states, actions, transition_probs, rewards, policy, discount_factor)\n",
    "\n",
    "# Calculate expected rewards and transition probabilities\n",
    "expected_rewards = mrp.calculate_expected_reward()\n",
    "expected_transition_probs = mrp.calculate_expected_transition_probs()\n",
    "\n",
    "print(\"Expected rewards:\", expected_rewards)\n",
    "print(\"Expected transition probabilities:\", expected_transition_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : Code implication, Value Function, Bellman Expectation Eqn (matrix), Optimal value function, optimal policy, finding optimal policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
