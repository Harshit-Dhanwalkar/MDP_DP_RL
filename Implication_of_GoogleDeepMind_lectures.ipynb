{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implication of Google DeepMind Lectures\n",
    "\n",
    "This notebook will discuss the implications of the Google DeepMind lectures on MDP and Dynamic Programming. We will cover the following topics:\n",
    "\n",
    "1. Introduction markov property, markov Reward Process(MRP), etc.\n",
    "2. Futher into Value function\n",
    "3. Bellman euation for MRPs, solving Bellman euations.\n",
    "4. Markov Decision Process(MDP)\n",
    "5. Policy, and more about Value fucntion\n",
    "6. Bellman Expectation Equation(BEE), BEE for $V^\\pi$, $Q^\\pi$ and $V_\\pi(2)$\n",
    "7.  Bellman Expectation Equation (Matrix form)\n",
    "8. Optimal Value Function\n",
    "9. Optimal Policy, finding optimal policy\n",
    "10. Bellman optimality Equation(BOE), BOE for $V_*$ and $Q_*$.\n",
    "11. Solving for Bellman Optimatity Equation\n",
    "\n",
    "Furthur more....\n",
    "1. Real-world applications and implications\n",
    "2. Future directions and challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Can furthur be appended for RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The Markov property is a fundamental concept in probability theory and stochastic processes. It states that the future behavior of a system depends only on its current state and is independent of its past history, given the present state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Def : State $S_t$ is markov iff\n",
    "$$P[S_{t+1}|S_t]=P[S_{t+1}|S_1...S_t]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Transition matrix ($P$)\n",
    "State $s$ and successor state $s'$ and State transition probability $P_{ss'}=P[S_{t+1}=s'|S_t=s]$\n",
    "\n",
    "State transition matrix $P$ represents the probabilities of transitioning from one state to another in a Markov Decision Process (MDP). It is defined as:\n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "p_{11} & p_{12} & \\dots & p_{1n} \\\\\n",
    "p_{21} & p_{22} & \\dots & p_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "p_{n1} & p_{n2} & \\dots & p_{nn} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $p_{ij}$ represents the probability of transitioning from state $i$ to state $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Reward Process (MRPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an MRP, we have a set of states and a set of possible actions that can be taken in each state. When an action is taken in a state, the system transitions to a new state according to a transition probability distribution. Additionally, each state transition is associated with a reward.\n",
    "\n",
    "Formally, an MRP is defined as a tuple (S, P, R, γ), where:\n",
    "- S is the set of states.$$S = \\{s_1,s_2,...s_N\\}$$\n",
    "- P is the state transition probability matrix, where P(s, s') represents the probability of transitioning from state s to state s'.$$P_{ss'}=P[S_{t+1}=s'|S_t=s]$$\n",
    "- R is the reward function, where R(s, s') represents the reward obtained when transitioning from state s to state s'.$$R_{s}=E[R_{t+1}|S_t=s]$$\n",
    "- γ is the discount factor, which determines the importance of future rewards compared to immediate rewards. $$\\gamma \\in [0,1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal in an MRP is to find a policy that maximizes the expected cumulative reward over time. This can be achieved by solving the Bellman equations, which provide a way to compute the value function for each state. The value function represents the expected cumulative reward starting from a given state and \n",
    "following a specific policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Def : $G_t$ is the total discounted reward from the step function.$$G_t = R_{t+1}+\\gamma R_{t+2}+\\cdots=\\sum^{\\infty}_{k=0}\\gamma^kR_{t+k+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value immediate reward above delayed reward\n",
    "1. $\\gamma$ close to 0 leads to \"myopic\" evalution.\n",
    "2. $\\gamma$ close to 1 leads to \"Far-sighted\" evalution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function ($V(s)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function $V(s)$ gives the long-term value of state $s$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Def : State value function $V(s)$ of an MRP is the expected return starting from state $s$ $$V(s)=E[G_t|S_t=s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman equation for MRPs\n",
    "The value function can be decomposed into 2 parts:\n",
    "- immediate rewards $R_{t+1}$.\n",
    "- disounted value of successor state $\\gamma v(S_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "V(s) &= \\mathbb{E}[G_t|S_t=s] \\\\\n",
    "     &= \\mathbb{E}[R_{t+1}\\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots|S_t=s] \\\\\n",
    "     &= \\mathbb{E}[R_{t+1}\\gamma [R_{t+2} + \\gamma R_{t+3} + \\cdots]|S_t=s] \\\\\n",
    "     &= \\mathbb{E}[R_{t+1}\\gamma G_{t+1}|S_t=s]\\\\\n",
    "     &= \\mathbb{E}[R_{t+1}\\gamma v(s_{t+1})|S_t=s]\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bellman equation can be expressed using matrix\n",
    "The Bellman equation represents the relationship between the value of a state ($v$), the immediate reward ($R$) obtained in that state, the discount factor ($\\gamma$), and the value of the next state ($Pv$).\n",
    "\n",
    "In the equation, $v$ is a vector representing the values of all possible states in a given environment. $R$ is a vector containing the immediate rewards associated with each state. $\\gamma$  is a scalar value between 0 and 1 that determines the importance of future rewards compared to immediate rewards. $P$ is a transition matrix that describes the probability of transitioning from one state to another.\n",
    "\n",
    "we can write it as:\n",
    "\n",
    "$$v = R + \\gamma Pv$$\n",
    "\n",
    "Here, $v$, $R$, and $Pv$ are all column vectors. The matrix multiplication $Pv$ represents the weighted sum of the values of the next states, where the weights are determined by the transition probabilities in the matrix $P$. The scalar $\\gamma$  determines the discounting factor applied to the future rewards. this can be rewiten as :\n",
    "\n",
    "\\begin{bmatrix} V^{(1)} \\\\\\vdots \\\\V^{(n)}  \\end{bmatrix} = \\begin{bmatrix}   R_{1}\\\\ \\vdots \\\\ R_{n} \\end{bmatrix} + \\gamma \\begin{bmatrix}   P_{11} & \\cdots & P_{1n} \\\\    \\vdots &      & \\vdots \\\\    P_{n1} & \\cdots & P_{nn} \\end{bmatrix} \\begin{bmatrix} V^{(1)} \\\\  \\vdots \\\\  V^{(n)} \\end{bmatrix}\n",
    "\n",
    "To solve the Bellman equation, we can rearrange it as:\n",
    "\n",
    "$$(I - \\gamma P)v = R$$\n",
    "\n",
    "Where $I$ is the identity matrix. By solving this equation, we can find the values of all states in the environment, which can be used to make optimal decisions in a reinforcement learning setting.\n",
    "\n",
    "It's important to note that the dimensions of the matrices and vectors involved in the equation must be compatible for matrix operations. The number of rows in $P$ should be equal to the number of columns in $v$ and $R$. Additionally, the dimensions of $\\gamma Pv$ and $R$ should match for the addition operation to be valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving Bellman equation\n",
    " - They are linear equations.\n",
    " - Solved directly by :\n",
    "       $$v= R+\\gamma Pv$$ $$(1-\\gamma P)v =R$$ $$v = (1-\\gamma P)^{-1}R$$\n",
    " - Computational complexcity is $O(n^3)$ for $n$ states.\n",
    " - Direct solution only possible for small MRPs.\n",
    " - There are many iterative methods for large MDPs, for example: \\\n",
    "       - Dynamic programming \\\n",
    "       - Monte-Carlo evalution \\\n",
    "       - Temporal-Difference learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process (MDPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Decision process is a tuple $<S,A,P,R,\\gamma>$\n",
    "- S is a finite set of states. $$S = \\{s_1, s_2, \\ldots, s_n\\}$$\n",
    "- A is a finite set of actions. $$A = \\{a_1, a_2, \\ldots, a_m\\}$$\n",
    "- P is the state transition probability matrix, where $P(s, a, s')$ represents the probability of transitioning from state $s$ to state $s'$ when taking action $a$. $$P^a_{ss'} = P[S_{t+1}=s'|S_t=s, A_t=a]$$\n",
    "- R is the reward function, where $R(s, a)$ represents the reward obtained when taking action $a$ in state $s$. $$R^a_s = \\mathbb{E}[R_{t+1}|S_t=s, A_t=a]$$\n",
    "- $\\gamma$ is the discount factor, which determines the importance of future rewards compared to immediate rewards. It is a scalar value between 0 and 1, i.e, $$\\gamma \\in [0,1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block, class represents a Markov Decision Process (MDP).\n",
    "\n",
    "A Markov Decision Process is a mathematical framework used to model decision-making problems in situations where outcomes are partly random and partly under the control of a decision-maker. It consists of a set of states, a set of actions, transition probabilities, and rewards associated with state-action pairs.\n",
    "\n",
    "Attributes:\n",
    "- states (list): A list of all possible states in the MDP.\n",
    "- actions (list): A list of all possible actions in the MDP.\n",
    "- transition_probs (dict): A dictionary that maps state-action pairs to a list of tuples representing the next state and the corresponding transition probability.\n",
    "- rewards (dict): A dictionary that maps state-action pairs to the corresponding reward.\n",
    "\n",
    "Methods:\n",
    "- get_transition_prob(state, action, next_state): Returns the transition probability from a given state to the next state, given a specific action.\n",
    "- get_reward(state, action): Returns the reward associated with a given state-action pair.\n",
    "- get_possible_actions(state): Returns a list of possible actions from a given state.\n",
    "- get_possible_states(): Returns a list of all possible states in the MDP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess:\n",
    "    def __init__(self, states, actions, transition_probs, rewards):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transition_probs = transition_probs\n",
    "        self.rewards = rewards\n",
    "\n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        return self.transition_probs[(state, action)][next_state]\n",
    "\n",
    "    def get_reward(self, state, action):\n",
    "        return self.rewards[(state, action)]\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        return [action for action in self.actions if (state, action) in self.transition_probs]\n",
    "\n",
    "    def get_possible_states(self):\n",
    "        return self.states       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy  ($\\pi$)\n",
    "Def : A policy $\\pi$ is distribution over actions given states. $$\\pi(a|s)= P[A_t=a|S_t=s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given MDP : M = $<S,A,P,R,\\gamma>$ and policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- State sequence $s_1, s_2 \\cdots $ is markov process $<S,P^\\pi>$\n",
    "- The state & reward sequence $S_1,R_2,S_2, \\cdots $ is a markov Reward process $<S,P^\\pi,R^\\pi, \\gamma>$\n",
    "- $$P^\\pi_{ss'}=\\sum_{a \\in A}\\pi(a|s)P^a_{ss'}$$\n",
    "$$R^\\pi_{s}=\\sum_{a \\in A}\\pi(a|s)R^a_{s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : Code implication, Value Function, Bellman Expectation Eqn (matrix), Optimal value function, optimal policy, finding optimal policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
